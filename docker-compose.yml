

services:
  # Redis for Celery Broker and Backend
  redis:
    image: redis:7-alpine
    container_name: ocriador-redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  # LLM Backend API (FastAPI)
  llm_api:
    build:
      context: ./llm-backend
      dockerfile: Dockerfile
    container_name: ocriador-llm-api
    depends_on:
      - redis
    # Do not expose ports to the host, as traffic will go through the tunnel
    # ports:
    #   - "8000:8000"
    volumes:
      # Mount the models directory
      - ./llm-backend/models:/app/models
      # Mount the app code for live reloading during development
      - ./llm-backend/app:/app/app
    env_file:
      - .env
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    restart: unless-stopped

  # LLM Worker (Celery)
  llm_worker:
    build:
      context: ./llm-backend
      dockerfile: Dockerfile
    container_name: ocriador-llm-worker
    depends_on:
      - redis
    volumes:
      # Mount the models directory
      - ./llm-backend/models:/app/models
      # Mount the app code for live reloading during development
      - ./llm-backend/app:/app/app
    env_file:
      - .env
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    # Command to start the celery worker
    command: ["celery", "-A", "app.celery_worker.celery_app", "worker", "--loglevel=info", "--concurrency=1"]
    restart: unless-stopped

  # Cloudflare Tunnel
  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: ocriador-cloudflared
    depends_on:
      - llm_api
    restart: unless-stopped
    command: tunnel --no-autoupdate run --token ${TUNNEL_TOKEN}
